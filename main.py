import os
import re
import tempfile
import time
import threading
from flask import Flask, request, abort
from dotenv import load_dotenv
from openai import OpenAI
import google.generativeai as genai
import requests
from bs4 import BeautifulSoup
from notion_client import Client as NotionClient

from apify_client import ApifyClient

from linebot.v3 import WebhookHandler
from linebot.v3.exceptions import InvalidSignatureError
from linebot.v3.messaging import (
    Configuration,
    ApiClient,
    MessagingApi,
    MessagingApiBlob,
    ReplyMessageRequest,
    PushMessageRequest,
    TextMessage,
    QuickReply,
    QuickReplyItem,
    MessageAction,
)
from linebot.v3.webhooks import MessageEvent, TextMessageContent, AudioMessageContent

load_dotenv()

app = Flask(__name__)

CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN")
CHANNEL_SECRET = os.getenv("LINE_CHANNEL_SECRET")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
NOTION_API_KEY = os.getenv("NOTION_API_KEY")
NOTION_DATABASE_ID = os.getenv("NOTION_DATABASE_ID")
NOTION_SOCIAL_DATABASE_ID = os.getenv("NOTION_SOCIAL_DATABASE_ID")
APIFY_API_KEY = os.getenv("APIFY_API_KEY")

if not CHANNEL_ACCESS_TOKEN or not CHANNEL_SECRET:
    raise ValueError("Please set LINE_CHANNEL_ACCESS_TOKEN and LINE_CHANNEL_SECRET in .env file")

configuration = Configuration(access_token=CHANNEL_ACCESS_TOKEN)
handler = WebhookHandler(CHANNEL_SECRET)

# OpenAI client for Whisper
openai_client = None
if OPENAI_API_KEY:
    openai_client = OpenAI(api_key=OPENAI_API_KEY)

# Gemini client for text processing
gemini_model = None
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    gemini_model = genai.GenerativeModel('gemini-2.0-flash')

# Notion client for saving content
notion_client = None
if NOTION_API_KEY and NOTION_DATABASE_ID:
    notion_client = NotionClient(auth=NOTION_API_KEY)
    print("[DEBUG] Notion client initialized")

# Apify client for social media scraping
apify_client = None
if APIFY_API_KEY:
    apify_client = ApifyClient(APIFY_API_KEY)
    print("[DEBUG] Apify client initialized")

# User states for translation mode (in-memory storage)
# Structure: { user_id: { "mode": "translate", "target_language": "English", "entered_at": timestamp } }
user_states = {}

# Translation mode timeout (5 minutes)
TRANSLATION_MODE_TIMEOUT = 5 * 60  # 5 minutes in seconds


def check_translation_timeout():
    """Background thread to check and handle translation mode timeouts"""
    while True:
        try:
            current_time = time.time()
            users_to_remove = []

            # Find users who have timed out
            for user_id, state in list(user_states.items()):
                if state.get("mode") in ["translate_waiting", "translate_select_language"]:
                    entered_at = state.get("entered_at", current_time)
                    if current_time - entered_at >= TRANSLATION_MODE_TIMEOUT:
                        users_to_remove.append(user_id)

            # Remove timed out users and send notification
            for user_id in users_to_remove:
                if user_id in user_states:
                    del user_states[user_id]
                    print(f"[DEBUG] User {user_id} translation mode timed out")

                    # Send push message to notify user
                    try:
                        with ApiClient(configuration) as api_client:
                            messaging_api = MessagingApi(api_client)
                            messaging_api.push_message(
                                PushMessageRequest(
                                    to=user_id,
                                    messages=[TextMessage(text="â° ç¿»è­¯æ¨¡å¼å·²é€¾æ™‚ï¼ˆ5åˆ†é˜ï¼‰ï¼Œå·²è‡ªå‹•é€€å‡ºã€‚\n\nå¦‚éœ€ç¹¼çºŒç¿»è­¯ï¼Œè«‹é‡æ–°è¼¸å…¥ã€Œç¿»è­¯ã€é€²å…¥ç¿»è­¯æ¨¡å¼ã€‚")]
                                )
                            )
                            print(f"[DEBUG] Timeout notification sent to user {user_id}")
                    except Exception as e:
                        print(f"[DEBUG] Failed to send timeout notification: {str(e)}")

        except Exception as e:
            print(f"[DEBUG] Error in timeout checker: {str(e)}")

        # Check every 30 seconds
        time.sleep(30)


# Start background thread for timeout checking
timeout_thread = threading.Thread(target=check_translation_timeout, daemon=True)
timeout_thread.start()

# URL pattern for detecting links
URL_PATTERN = re.compile(
    r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+[/\w\.-]*(?:\?[^\s]*)?'
)

# Social media URL patterns
# Single post patterns
FACEBOOK_POST_PATTERN = re.compile(
    r'https?://(?:www\.|m\.|web\.)?facebook\.com/(?:[\w.]+/)?(?:posts|videos|photos|watch|story\.php|permalink\.php|reel|share)[\w/?=&.-]*'
)
# Facebook page/profile patterns (for multi-post scraping)
FACEBOOK_PAGE_PATTERN = re.compile(
    r'https?://(?:www\.|m\.|web\.)?facebook\.com/([\w.]+)/?(?:\?.*)?$'
)
THREADS_PATTERN = re.compile(
    r'https?://(?:www\.)?threads\.net/@[\w.]+/post/[\w]+'
)

# Command pattern for multi-post scraping: "çˆ¬ 5 ç¯‡ [URL]" or "å¹«æˆ‘çˆ¬ 10 ç¯‡ [URL]"
SCRAPE_MULTI_PATTERN = re.compile(
    r'^(?:å¹«æˆ‘)?çˆ¬å–?\s*(\d+)\s*ç¯‡\s*(https?://\S+)',
    re.IGNORECASE
)

# Translation pattern - matches various formats:
# ç¿»è­¯æˆè‹±æ–‡ï¼šä½ å¥½ / ç¿»è­¯æˆè‹±æ–‡:ä½ å¥½ / ç¿»è­¯æˆè‹±æ–‡ ä½ å¥½ / ç¿»è­¯è‹±æ–‡ï¼šä½ å¥½
# å¹«æˆ‘ç¿»è­¯æˆè‹±æ–‡ï¼šä½ å¥½ / è«‹ç¿»è­¯æˆæ—¥æ–‡ï¼šä½ å¥½ / å¹«æˆ‘ç¿»è­¯æˆè¶Šå—æ–‡ ä½ å¥½
TRANSLATE_PATTERN = re.compile(
    r'^(?:å¹«æˆ‘|è«‹|è«‹å¹«æˆ‘)?ç¿»è­¯æˆ?\s*(.+?)\s*[ï¼š:\s]\s*(.+)$',
    re.DOTALL
)

# Quick Reply language options for translation mode
QUICK_REPLY_LANGUAGES = [
    ("è‹±æ–‡", "English"),
    ("æ—¥æ–‡", "Japanese"),
    ("éŸ“æ–‡", "Korean"),
    ("è¶Šå—æ–‡", "Vietnamese"),
    ("æ³°æ–‡", "Thai"),
    ("å°å°¼æ–‡", "Indonesian"),
    ("ç°¡é«”ä¸­æ–‡", "Simplified Chinese"),
    ("æ³•æ–‡", "French"),
    ("è¥¿ç­ç‰™æ–‡", "Spanish"),
    ("å¾·æ–‡", "German"),
]

# Language name mapping (Chinese name -> language code for OpenAI)
LANGUAGE_MAP = {
    # å¸¸ç”¨èªè¨€
    "è‹±æ–‡": "English",
    "è‹±èª": "English",
    "æ—¥æ–‡": "Japanese",
    "æ—¥èª": "Japanese",
    "éŸ“æ–‡": "Korean",
    "éŸ“èª": "Korean",
    "ä¸­æ–‡": "Traditional Chinese",
    "ç¹é«”ä¸­æ–‡": "Traditional Chinese",
    "ç¹ä¸­": "Traditional Chinese",
    "ç°¡é«”ä¸­æ–‡": "Simplified Chinese",
    "ç°¡ä¸­": "Simplified Chinese",
    # æ±å—äºèªè¨€
    "è¶Šå—æ–‡": "Vietnamese",
    "è¶Šå—èª": "Vietnamese",
    "æ³°æ–‡": "Thai",
    "æ³°èª": "Thai",
    "å°å°¼æ–‡": "Indonesian",
    "å°å°¼èª": "Indonesian",
    "é¦¬ä¾†æ–‡": "Malay",
    "é¦¬ä¾†èª": "Malay",
    "è²å¾‹è³“æ–‡": "Filipino",
    "è²å¾‹è³“èª": "Filipino",
    "ç·¬ç”¸æ–‡": "Burmese",
    "ç·¬ç”¸èª": "Burmese",
    "æŸ¬åŸ”å¯¨æ–‡": "Khmer",
    "æŸ¬åŸ”å¯¨èª": "Khmer",
    "é«˜æ£‰æ–‡": "Khmer",
    "å¯®æ–‡": "Lao",
    "å¯®èª": "Lao",
    "å¯®åœ‹æ–‡": "Lao",
    # æ­æ´²èªè¨€
    "æ³•æ–‡": "French",
    "æ³•èª": "French",
    "å¾·æ–‡": "German",
    "å¾·èª": "German",
    "è¥¿ç­ç‰™æ–‡": "Spanish",
    "è¥¿ç­ç‰™èª": "Spanish",
    "è‘¡è„ç‰™æ–‡": "Portuguese",
    "è‘¡è„ç‰™èª": "Portuguese",
    "ç¾©å¤§åˆ©æ–‡": "Italian",
    "ç¾©å¤§åˆ©èª": "Italian",
    "ä¿„æ–‡": "Russian",
    "ä¿„èª": "Russian",
    "è·è˜­æ–‡": "Dutch",
    "è·è˜­èª": "Dutch",
    # å…¶ä»–èªè¨€
    "é˜¿æ‹‰ä¼¯æ–‡": "Arabic",
    "é˜¿æ‹‰ä¼¯èª": "Arabic",
    "å°åº¦æ–‡": "Hindi",
    "å°åœ°èª": "Hindi",
    "åœŸè€³å…¶æ–‡": "Turkish",
    "åœŸè€³å…¶èª": "Turkish",
    "æ³¢è˜­æ–‡": "Polish",
    "æ³¢è˜­èª": "Polish",
    "ç‘å…¸æ–‡": "Swedish",
    "ç‘å…¸èª": "Swedish",
    "å¸Œè‡˜æ–‡": "Greek",
    "å¸Œè‡˜èª": "Greek",
}


def extract_url(text: str) -> str | None:
    """Extract the first URL from text"""
    match = URL_PATTERN.search(text)
    return match.group(0) if match else None


def detect_social_platform(url: str) -> tuple[str | None, str]:
    """Detect social media platform type and URL type

    Returns:
        Tuple of (platform, url_type) where url_type is "post" or "page"
    """
    if FACEBOOK_POST_PATTERN.match(url):
        return ("facebook", "post")
    if FACEBOOK_PAGE_PATTERN.match(url):
        return ("facebook", "page")
    if THREADS_PATTERN.match(url):
        return ("threads", "post")
    return (None, "")


def scrape_facebook_post(url: str, max_posts: int = 1) -> list[dict]:
    """Scrape Facebook post(s) using Apify

    Args:
        url: Facebook URL (post or page)
        max_posts: Maximum number of posts to scrape (default 1)

    Returns:
        List of post data dictionaries
    """
    if not apify_client:
        print("[DEBUG] Apify client not configured")
        return []

    try:
        print(f"[DEBUG] Scraping Facebook URL: {url}, max_posts: {max_posts}")
        run_input = {
            "startUrls": [{"url": url}],
            "resultsLimit": max_posts,
        }
        run = apify_client.actor("apify/facebook-posts-scraper").call(run_input=run_input)
        items = list(apify_client.dataset(run["defaultDatasetId"]).iterate_items())
        if items:
            print(f"[DEBUG] Facebook scrape successful, got {len(items)} posts")
            return items
        print("[DEBUG] No items returned from Facebook scraper")
        return []
    except Exception as e:
        print(f"[DEBUG] Facebook scrape error: {str(e)}")
        return []


def scrape_threads_post(url: str, max_posts: int = 1) -> list[dict]:
    """Scrape Threads post(s) using Apify

    Args:
        url: Threads URL
        max_posts: Maximum number of posts to scrape (default 1)

    Returns:
        List of post data dictionaries
    """
    if not apify_client:
        print("[DEBUG] Apify client not configured")
        return []

    try:
        print(f"[DEBUG] Scraping Threads post: {url}")
        run_input = {
            "url": url,
        }
        run = apify_client.actor("sinam7/threads-post-scraper").call(run_input=run_input)
        items = list(apify_client.dataset(run["defaultDatasetId"]).iterate_items())
        if items:
            print(f"[DEBUG] Threads scrape successful, got {len(items)} posts")
            return items
        print("[DEBUG] No items returned from Threads scraper")
        return []
    except Exception as e:
        print(f"[DEBUG] Threads scrape error: {str(e)}")
        return []


def scrape_google_maps(url: str) -> dict | None:
    """Scrape Google Maps place data using Apify

    Args:
        url: Google Maps URL

    Returns:
        Place data dictionary or None
    """
    if not apify_client:
        print("[DEBUG] Apify client not configured for Google Maps scraping")
        return None

    try:
        print(f"[DEBUG] Scraping Google Maps URL: {url}")
        run_input = {
            "startUrls": [{"url": url}],
        }
        run = apify_client.actor("blueorion/free-google-maps-scraper-extensive").call(run_input=run_input)
        items = list(apify_client.dataset(run["defaultDatasetId"]).iterate_items())
        if items:
            print(f"[DEBUG] Google Maps scrape successful, got {len(items)} places")
            return items[0]
        print("[DEBUG] No items returned from Google Maps scraper")
        return None
    except Exception as e:
        print(f"[DEBUG] Google Maps scrape error: {str(e)}")
        return None


def format_google_maps_result(place: dict) -> str:
    """Format scraped Google Maps place data into a readable summary"""
    lines = []

    name = place.get("title") or place.get("name") or "æœªçŸ¥åœ°é»"
    lines.append(f"ğŸ“ åœ°é»åç¨±ï¼š{name}")

    # Category / type
    category = place.get("categoryName") or place.get("category") or ""
    if category:
        lines.append(f"ğŸ·ï¸ é¡å‹ï¼š{category}")

    # Address
    address = place.get("address") or place.get("street") or ""
    if address:
        lines.append(f"ğŸ“® åœ°å€ï¼š{address}")

    # Rating
    rating = place.get("totalScore") or place.get("rating") or place.get("stars")
    reviews_count = place.get("reviewsCount") or place.get("reviews") or 0
    if rating:
        lines.append(f"â­ è©•åˆ†ï¼š{rating}/5ï¼ˆ{reviews_count} å‰‡è©•è«–ï¼‰")

    # Phone
    phone = place.get("phone") or place.get("phoneUnformatted") or ""
    if phone:
        lines.append(f"ğŸ“ é›»è©±ï¼š{phone}")

    # Website
    website = place.get("website") or place.get("url") or ""
    if website:
        lines.append(f"ğŸŒ ç¶²ç«™ï¼š{website}")

    # Price level
    price = place.get("price") or place.get("priceLevel") or ""
    if price:
        lines.append(f"ğŸ’° åƒ¹ä½ï¼š{price}")

    # Opening hours
    hours = place.get("openingHours")
    if hours:
        if isinstance(hours, list):
            lines.append("ğŸ• ç‡Ÿæ¥­æ™‚é–“ï¼š")
            for h in hours[:7]:
                if isinstance(h, dict):
                    day = h.get("day", "")
                    time_str = h.get("hours", "")
                    lines.append(f"  â€¢ {day}ï¼š{time_str}")
                elif isinstance(h, str):
                    lines.append(f"  â€¢ {h}")
        elif isinstance(hours, str):
            lines.append(f"ğŸ• ç‡Ÿæ¥­æ™‚é–“ï¼š{hours}")

    # Description
    description = place.get("description") or ""
    if description:
        lines.append(f"\nğŸ“ ç°¡ä»‹ï¼š{description}")

    # Location coordinates
    lat = place.get("location", {}).get("lat") or place.get("latitude")
    lng = place.get("location", {}).get("lng") or place.get("longitude")
    if lat and lng:
        lines.append(f"ğŸ—ºï¸ åº§æ¨™ï¼š{lat}, {lng}")

    # Additional info
    additional = place.get("additionalInfo") or place.get("additionalCategories")
    if additional and isinstance(additional, dict):
        for key, value in list(additional.items())[:5]:
            if value:
                lines.append(f"â„¹ï¸ {key}ï¼š{value}")

    return "\n".join(lines)


def setup_notion_social_database():
    """Initialize Notion social database with required properties"""
    if not notion_client or not NOTION_SOCIAL_DATABASE_ID:
        print("[DEBUG] Notion not configured for social database setup")
        return False

    try:
        # Update database with required properties
        notion_client.databases.update(
            database_id=NOTION_SOCIAL_DATABASE_ID,
            title=[{"text": {"content": "ç¤¾ç¾¤åˆ†æ"}}],
            properties={
                "åç¨±": {"title": {}},
                "å¹³å°": {
                    "select": {
                        "options": [
                            {"name": "Facebook", "color": "blue"},
                            {"name": "Threads", "color": "purple"},
                        ]
                    }
                },
                "å¸³è™Ÿ": {"rich_text": {}},
                "å…§å®¹æ‘˜è¦": {"rich_text": {}},
                "åŸå§‹å…§å®¹": {"rich_text": {}},
                "é—œéµå­—": {"multi_select": {"options": []}},
                "Likes": {"number": {"format": "number"}},
                "ç•™è¨€æ•¸": {"number": {"format": "number"}},
                "åˆ†äº«æ•¸": {"number": {"format": "number"}},
                "ä¾†æºç¶²å€": {"url": {}},
                "é¡å‹": {
                    "select": {
                        "options": [
                            {"name": "è³‡è¨Šåˆ†äº«", "color": "blue"},
                            {"name": "å€‹äººå¿ƒå¾—", "color": "green"},
                            {"name": "ç”¢å“æ¨å»£", "color": "orange"},
                            {"name": "æ–°èå ±å°", "color": "red"},
                            {"name": "æ•™å­¸å…§å®¹", "color": "yellow"},
                            {"name": "å¨›æ¨‚å…§å®¹", "color": "pink"},
                            {"name": "æ´»å‹•å®£å‚³", "color": "purple"},
                            {"name": "å…¶ä»–", "color": "gray"},
                        ]
                    }
                },
                "LINE ç”¨æˆ¶": {"rich_text": {}},
                "å»ºç«‹æ™‚é–“": {"created_time": {}},
            }
        )
        print("[DEBUG] Notion social database setup completed")
        return True
    except Exception as e:
        print(f"[DEBUG] Notion database setup error: {str(e)}")
        return False


def normalize_social_post_data(post_data: dict, platform: str) -> dict:
    """Normalize post data from different platforms to a common format"""
    print(f"[DEBUG] Raw post data: {post_data}")

    if platform == "facebook":
        # Try various field names from Apify Facebook scraper
        user_dict = post_data.get("user") if isinstance(post_data.get("user"), dict) else {}
        username = (
            post_data.get("pageName") or
            post_data.get("userName") or
            user_dict.get("name") or
            post_data.get("name") or
            "æœªçŸ¥"
        )
        text = (
            post_data.get("text") or
            post_data.get("postText") or
            post_data.get("message") or
            post_data.get("description") or
            ""
        )
        # Handle various reaction/like field names
        reactions_dict = post_data.get("reactions") if isinstance(post_data.get("reactions"), dict) else {}
        likes = (
            post_data.get("likes") or
            post_data.get("likesCount") or
            post_data.get("reactionsCount") or
            reactions_dict.get("count") or
            0
        )
        comments = (
            post_data.get("comments") or
            post_data.get("commentsCount") or
            post_data.get("commentCount") or
            0
        )
        shares = (
            post_data.get("shares") or
            post_data.get("sharesCount") or
            post_data.get("shareCount") or
            0
        )
        return {
            "username": username,
            "text": text,
            "likes": int(likes) if likes else 0,
            "comments": int(comments) if comments else 0,
            "shares": int(shares) if shares else 0,
        }
    elif platform == "threads":
        return {
            "username": post_data.get("ownerUsername") or post_data.get("author", {}).get("username") or "æœªçŸ¥",
            "text": post_data.get("text") or post_data.get("caption") or "",
            "likes": int(post_data.get("likeCount") or post_data.get("likesCount") or 0),
            "comments": int(post_data.get("replyCount") or post_data.get("commentsCount") or 0),
            "shares": int(post_data.get("repostCount") or 0),
        }
    return {
        "username": "æœªçŸ¥",
        "text": "",
        "likes": 0,
        "comments": 0,
        "shares": 0,
    }


def summarize_social_post(post_data: dict, platform: str) -> str:
    """Use AI to analyze social media post"""
    if not openai_client:
        return "ç¤¾ç¾¤åˆ†æåŠŸèƒ½æœªè¨­å®šï¼Œè«‹è¨­å®š OPENAI_API_KEY"

    platform_name = "Facebook" if platform == "facebook" else "Threads"

    try:
        prompt = f"""åˆ†æä»¥ä¸‹ {platform_name} è²¼æ–‡ï¼š
å¸³è™Ÿï¼š{post_data.get('username', 'æœªçŸ¥')}
å…§å®¹ï¼š{post_data.get('text', '')}
äº’å‹•æ•¸æ“šï¼š{post_data.get('likes', 0)} è®šã€{post_data.get('comments', 0)} ç•™è¨€ã€{post_data.get('shares', 0)} åˆ†äº«

è«‹ç”¨ä»¥ä¸‹æ ¼å¼å›è¦†ï¼ˆç¹é«”ä¸­æ–‡ï¼‰ï¼š

ğŸ“Œ å¸³è™Ÿï¼š{post_data.get('username', 'æœªçŸ¥')}

ğŸ“ æ‘˜è¦ï¼š[ç”¨2-3å¥è©±æ‘˜è¦è²¼æ–‡å…§å®¹çš„é‡é»]

ğŸ”‘ é—œéµå­—ï¼š[3-5å€‹é—œéµå­—ï¼Œç”¨é “è™Ÿåˆ†éš”]

ğŸ“Š äº’å‹•æ•¸æ“šï¼š{post_data.get('likes', 0)} è®š | {post_data.get('comments', 0)} ç•™è¨€ | {post_data.get('shares', 0)} åˆ†äº«

ğŸ¯ è²¼æ–‡é¡å‹ï¼š[åªé¸ä¸€å€‹ï¼šè³‡è¨Šåˆ†äº«ã€å€‹äººå¿ƒå¾—ã€ç”¢å“æ¨å»£ã€æ–°èå ±å°ã€æ•™å­¸å…§å®¹ã€å¨›æ¨‚å…§å®¹ã€æ´»å‹•å®£å‚³ã€å…¶ä»–]
"""
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç¤¾ç¾¤åª’é«”åˆ†æåŠ©æ‰‹ï¼Œæ“…é•·åˆ†æè²¼æ–‡å…§å®¹ä¸¦æå–é—œéµè³‡è¨Šã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1000,
            temperature=0.7
        )
        return response.choices[0].message.content

    except Exception as e:
        return f"ç¤¾ç¾¤åˆ†æå¤±æ•—ï¼š{str(e)}"


def parse_social_summary_response(response: str) -> dict:
    """Parse summary and keywords from social post AI response"""
    result = {
        "summary": "",
        "keywords": [],
        "post_type": "å…¶ä»–",
    }

    # Parse ğŸ“ æ‘˜è¦ï¼šxxx
    summary_match = re.search(r'ğŸ“\s*æ‘˜è¦[ï¼š:]\s*(.+?)(?:\n\n|ğŸ”‘|$)', response, re.DOTALL)
    if summary_match:
        result["summary"] = summary_match.group(1).strip()

    # Parse ğŸ”‘ é—œéµå­—ï¼šxxx
    keywords_match = re.search(r'ğŸ”‘\s*é—œéµå­—[ï¼š:]\s*(.+?)(?:\n\n|ğŸ“Š|$)', response, re.DOTALL)
    if keywords_match:
        keywords_text = keywords_match.group(1).strip()
        keywords = re.split(r'[ã€,ï¼Œ]', keywords_text)
        result["keywords"] = [kw.strip() for kw in keywords if kw.strip() and len(kw.strip()) < 50]

    # Parse ğŸ¯ è²¼æ–‡é¡å‹ï¼šxxx
    type_match = re.search(r'ğŸ¯\s*è²¼æ–‡é¡å‹[ï¼š:]\s*(.+?)(?:\n|$)', response)
    if type_match:
        result["post_type"] = type_match.group(1).strip()

    return result


def save_social_to_notion(
    platform: str,
    username: str,
    summary: str,
    original_text: str,
    keywords: list[str],
    likes: int,
    comments: int,
    shares: int,
    source_url: str,
    post_type: str = "å…¶ä»–",
    user_id: str = None
) -> bool:
    """Save social media post to Notion database

    Args:
        platform: "Facebook" | "Threads"
        username: Account name
        summary: AI-generated summary
        original_text: Original post content
        keywords: List of keywords
        likes: Like count
        comments: Comment count
        shares: Share count
        source_url: Original post URL
        post_type: Post type category
        user_id: LINE User ID

    Returns:
        True if saved successfully, False otherwise
    """
    if not notion_client:
        print("[DEBUG] Notion not configured, skipping save")
        return False

    # Use dedicated social database if available, otherwise use default
    database_id = NOTION_SOCIAL_DATABASE_ID or NOTION_DATABASE_ID
    if not database_id:
        print("[DEBUG] No Notion database ID configured")
        return False

    try:
        # Build title: account name + summary snippet
        title = f"[{platform}] {username}: {summary[:50]}..." if len(summary) > 50 else f"[{platform}] {username}: {summary}"

        properties = {
            "åç¨±": {"title": [{"text": {"content": title[:100]}}]},
            "å¹³å°": {"select": {"name": platform}},
            "å¸³è™Ÿ": {"rich_text": [{"text": {"content": username[:100]}}]},
            "å…§å®¹æ‘˜è¦": {"rich_text": [{"text": {"content": summary[:2000]}}]},
            "åŸå§‹å…§å®¹": {"rich_text": [{"text": {"content": original_text[:2000] if original_text else ""}}]},
            "ä¾†æºç¶²å€": {"url": source_url},
        }

        # Add keywords as multi-select
        if keywords:
            properties["é—œéµå­—"] = {"multi_select": [{"name": kw[:100]} for kw in keywords[:10]]}

        # Add numeric fields (only if the database has these columns)
        # These will be added if the database supports them
        try:
            properties["Likes"] = {"number": likes}
            properties["ç•™è¨€æ•¸"] = {"number": comments}
            properties["åˆ†äº«æ•¸"] = {"number": shares}
        except Exception:
            pass

        # Add category/type
        if post_type:
            properties["é¡å‹"] = {"select": {"name": post_type}}

        if user_id:
            properties["LINE ç”¨æˆ¶"] = {"rich_text": [{"text": {"content": user_id}}]}

        # Create page in database
        notion_client.pages.create(
            parent={"database_id": database_id},
            properties=properties
        )

        print(f"[DEBUG] Saved social post to Notion: {title[:50]}...")
        return True

    except Exception as e:
        print(f"[DEBUG] Notion save error: {str(e)}")
        return False


def fetch_webpage_content(url: str) -> str:
    """Fetch and extract key content from a webpage"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=5)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # Get title
        title = ""
        if soup.title:
            title = soup.title.string or ""

        # Get meta description
        description = ""
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            description = meta_desc.get('content', '')

        # Get og:description as fallback
        if not description:
            og_desc = soup.find('meta', attrs={'property': 'og:description'})
            if og_desc:
                description = og_desc.get('content', '')

        # Remove unnecessary elements
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe', 'noscript']):
            element.decompose()

        # Get article content
        article = soup.find('article') or soup.find('main') or soup.find('div', class_=lambda x: x and 'content' in x.lower() if x else False)

        if article:
            content = article.get_text(separator='\n', strip=True)
        else:
            content = soup.get_text(separator='\n', strip=True)

        # Clean up - remove extra whitespace and short lines
        lines = [line.strip() for line in content.split('\n') if len(line.strip()) > 20]
        content = '\n'.join(lines)

        # Limit content length
        if len(content) > 2000:
            content = content[:2000] + "..."

        return f"æ¨™é¡Œï¼š{title}\n\næè¿°ï¼š{description}\n\nå…§æ–‡ï¼š\n{content}"

    except Exception as e:
        return f"ç„¡æ³•æŠ“å–ç¶²é å…§å®¹ï¼š{str(e)}"


def summarize_webpage(content: str) -> str:
    """Use OpenAI to summarize webpage content"""
    if not openai_client:
        return "ç¶²é æ‘˜è¦åŠŸèƒ½æœªè¨­å®šï¼Œè«‹è¨­å®š OPENAI_API_KEY"

    try:
        prompt = f"""è«‹åˆ†æä»¥ä¸‹ç¶²é å…§å®¹ï¼Œç”¨ç¹é«”ä¸­æ–‡æä¾›å®Œæ•´æ‘˜è¦ï¼š

{content}

è«‹ç”¨ä»¥ä¸‹æ ¼å¼å›è¦†ï¼ˆæ¯å€‹æ¬„ä½åªå¡«ä¸€å€‹å€¼ï¼‰ï¼š

ğŸ·ï¸ åˆ†é¡ï¼š[åªé¸ä¸€å€‹ï¼šç§‘æŠ€ã€AIã€é‡‘èã€å•†æ¥­ã€æ–°èã€æ•™å­¸ã€é‹å‹•ã€ç¾é£Ÿã€æ—…éŠã€åœ°åœ–ã€ç”Ÿæ´»ã€å¨›æ¨‚ã€å…¶ä»–]

ğŸ“Œ ä¸»é¡Œï¼š[ä¸€å¥è©±æè¿°æ ¸å¿ƒä¸»é¡Œ]

ğŸ“ é‡é»æ‘˜è¦ï¼š
â€¢ [é‡é»1 - è©³ç´°èªªæ˜]
â€¢ [é‡é»2 - è©³ç´°èªªæ˜]
â€¢ [é‡é»3 - è©³ç´°èªªæ˜]
ï¼ˆä¾å…§å®¹æä¾›3-5å€‹é‡é»ï¼‰

ğŸ”‘ é—œéµå­—ï¼š[åˆ—å‡º3-5å€‹é—œéµå­—ï¼Œç”¨é “è™Ÿåˆ†éš”ï¼Œä¾‹å¦‚ï¼šäººå·¥æ™ºæ…§ã€ç¨‹å¼è¨­è¨ˆã€è‡ªå‹•åŒ–]

ğŸ¯ ä¸€å¥è©±ç¸½çµï¼š[ç”¨ä¸€å¥è©±ç¸½çµæ•´ç¯‡æ–‡ç« çš„æ ¸å¿ƒåƒ¹å€¼]
"""
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç¶²é æ‘˜è¦åŠ©æ‰‹ï¼Œæ“…é•·æå–é‡é»ä¸¦ç”¨ç¹é«”ä¸­æ–‡æ¸…æ™°å‘ˆç¾ã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1500,
            temperature=0.7
        )
        return response.choices[0].message.content

    except Exception as e:
        return f"æ‘˜è¦ç”Ÿæˆå¤±æ•—ï¼š{str(e)}"


def summarize_google_maps(content: str, url: str) -> str:
    """Use OpenAI to analyze Google Maps location"""
    if not openai_client:
        return "åœ°åœ–åˆ†æåŠŸèƒ½æœªè¨­å®šï¼Œè«‹è¨­å®š OPENAI_API_KEY"

    try:
        prompt = f"""è«‹åˆ†æä»¥ä¸‹ Google åœ°åœ–çš„åœ°é»è³‡è¨Šï¼Œç”¨ç¹é«”ä¸­æ–‡æä¾›åˆ†é¡å’Œæ‘˜è¦ï¼š

ç¶²å€ï¼š{url}
é é¢å…§å®¹ï¼š{content}

è«‹ç”¨ä»¥ä¸‹æ ¼å¼å›è¦†ï¼š

ğŸ·ï¸ åˆ†é¡ï¼šåœ°åœ–

ğŸ“ åœ°å€ï¼š[åœ‹å®¶/åŸå¸‚ï¼Œä¾‹å¦‚ï¼šæ—¥æœ¬æ±äº¬ã€è‡ºç£å°åŒ—ã€ç¾åœ‹ç´ç´„]

ğŸ½ï¸ é¡å‹ï¼š[å¦‚æœæ˜¯é¤å»³ï¼Œè«‹åˆ†é¡ï¼šæ—¥å¼ã€ç¾©å¼ã€ç¾å¼ã€æ³•å¼ã€ä¸­å¼ã€éŸ“å¼ã€æ³°å¼ã€è¶Šå—ã€å°åº¦ã€å¢¨è¥¿å“¥ã€æ­å¼ã€å’–å•¡å»³ã€é…’å§ã€ç”œé»ã€å…¶ä»–]
[å¦‚æœä¸æ˜¯é¤å»³ï¼Œè«‹èªªæ˜æ˜¯ä»€éº¼é¡å‹çš„åœ°é»ï¼šæ™¯é»ã€é£¯åº—ã€å•†åº—ã€å…¬å¸ã€ä½å®…ã€å…¶ä»–]

ğŸ“Œ åœ°é»åç¨±ï¼š[åº—åæˆ–åœ°é»åç¨±]

ğŸ“ é‡é»è³‡è¨Šï¼š
â€¢ [ç‡Ÿæ¥­æ™‚é–“ã€è©•åˆ†ã€åƒ¹ä½ç­‰è³‡è¨Šï¼Œå¦‚æœæœ‰çš„è©±]
â€¢ [ç‰¹è‰²æˆ–æ¨è–¦é …ç›®]
â€¢ [åœ°å€æˆ–äº¤é€šæ–¹å¼]

ğŸ”‘ é—œéµå­—ï¼š[åˆ—å‡º3-5å€‹é—œéµå­—ï¼Œç”¨é “è™Ÿåˆ†éš”ï¼Œä¾‹å¦‚ï¼šæ—¥æœ¬æ–™ç†ã€æ‹‰éºµã€æ±äº¬]

ğŸ¯ ä¸€å¥è©±ç¸½çµï¼š[ç°¡çŸ­æè¿°é€™å€‹åœ°é»]

æ³¨æ„ï¼šå¦‚æœç„¡æ³•å¾å…§å®¹åˆ¤æ–·æŸäº›è³‡è¨Šï¼Œè«‹æ¨™è¨»ã€Œç„¡æ³•åˆ¤æ–·ã€è€ŒéçŒœæ¸¬ã€‚
"""
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„åœ°é»åˆ†æåŠ©æ‰‹ï¼Œæ“…é•·å¾ Google åœ°åœ–è³‡è¨Šä¸­æå–åœ°é»é¡å‹ã€åœ°å€å’Œè©³ç´°è³‡è¨Šã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1000,
            temperature=0.5
        )
        return response.choices[0].message.content

    except Exception as e:
        return f"åœ°åœ–åˆ†æå¤±æ•—ï¼š{str(e)}"


# Known Whisper hallucination patterns
HALLUCINATION_PATTERNS = [
    "è¯·ä¸åç‚¹èµ",
    "é»è´Šè¨‚é–±",
    "è®¢é˜…è½¬å‘",
    "è¨‚é–±è½‰ç™¼",
    "æ‰“èµæ”¯æŒ",
    "æ‰“è³æ”¯æŒ",
    "æ˜é•œä¸ç‚¹ç‚¹",
    "æ˜é¡èˆ‡é»é»",
    "æ„Ÿè°¢è§‚çœ‹",
    "æ„Ÿè¬è§€çœ‹",
    "è°¢è°¢æ”¶çœ‹",
    "è¬è¬æ”¶çœ‹",
    "æ¬¢è¿è®¢é˜…",
    "æ­¡è¿è¨‚é–±",
    "like and subscribe",
    "thanks for watching",
    "å­—å¹•ç”±",
    "å­—å¹•æä¾›",
    "subtitles by",
    "amara.org",
]


def is_hallucination(text: str) -> bool:
    """Check if the transcription is likely a hallucination"""
    if not text or len(text.strip()) == 0:
        return True

    text_lower = text.lower().strip()

    # Check against known hallucination patterns
    for pattern in HALLUCINATION_PATTERNS:
        if pattern.lower() in text_lower:
            return True

    # Check if text is too short and repetitive
    if len(text_lower) < 5:
        return True

    # Check if text is just repeated characters/words
    words = text_lower.split()
    if len(words) > 2 and len(set(words)) == 1:
        return True

    return False


def parse_summary_response(response: str) -> dict:
    """Parse category and keywords from AI summary response"""
    result = {
        "category": "å…¶ä»–",
        "keywords": [],
        "title": ""
    }

    # Parse ğŸ·ï¸ åˆ†é¡ï¼šxxx
    category_match = re.search(r'ğŸ·ï¸\s*åˆ†é¡[ï¼š:]\s*(.+?)(?:\n|$)', response)
    if category_match:
        category = category_match.group(1).strip()
        # If category contains slash, take the first one
        if '/' in category:
            category = category.split('/')[0].strip()
        result["category"] = category

    # Parse ğŸ“Œ ä¸»é¡Œï¼šxxx or ğŸ“Œ åœ°é»åç¨±ï¼šxxx
    title_match = re.search(r'ğŸ“Œ\s*(?:ä¸»é¡Œ|åœ°é»åç¨±)[ï¼š:]\s*(.+?)(?:\n|$)', response)
    if title_match:
        result["title"] = title_match.group(1).strip()

    # Parse ğŸ”‘ é—œéµå­— or ğŸ’¡ é—œéµå­— or ğŸ’¡ é—œéµè³‡è¨Š
    keywords_match = re.search(r'[ğŸ”‘ğŸ’¡]\s*é—œéµ(?:å­—|è³‡è¨Š)[ï¼š:]\s*(.+?)(?:\n\n|ğŸ¯|$)', response, re.DOTALL)
    if keywords_match:
        keywords_text = keywords_match.group(1).strip()
        # Remove any newlines and clean up
        keywords_text = keywords_text.replace('\n', 'ã€')
        # Split by common separators: ã€,ï¼Œ
        keywords = re.split(r'[ã€,ï¼Œ]', keywords_text)
        # Clean up each keyword and filter empty ones
        result["keywords"] = [kw.strip() for kw in keywords if kw.strip() and len(kw.strip()) < 50]

    return result


def save_to_notion(
    title: str,
    content_type: str,
    category: str,
    content: str,
    source_url: str = None,
    original_text: str = None,
    keywords: list[str] = None,
    target_language: str = None,
    user_id: str = None
) -> bool:
    """Save content to Notion database

    Args:
        title: Main title/subject
        content_type: "URLæ‘˜è¦" | "èªéŸ³è½‰æ–‡å­—" | "ç¿»è­¯"
        category: Category from the predefined list
        content: Full summary/transcription/translation content
        source_url: Original URL (for URL summaries)
        original_text: Original text (for translations)
        keywords: List of keywords extracted by AI
        target_language: Target language (for translations)
        user_id: LINE User ID

    Returns:
        True if saved successfully, False otherwise
    """
    if not notion_client or not NOTION_DATABASE_ID:
        print("[DEBUG] Notion not configured, skipping save")
        return False

    try:
        # Build properties
        properties = {
            "æ¨™é¡Œ": {"title": [{"text": {"content": title[:100] if title else "ç„¡æ¨™é¡Œ"}}]},
            "é¡å‹": {"select": {"name": content_type}},
            "åˆ†é¡": {"select": {"name": category}},
            "å…§å®¹": {"rich_text": [{"text": {"content": content[:2000] if content else ""}}]},
        }

        # Add optional fields
        if source_url:
            properties["ä¾†æºç¶²å€"] = {"url": source_url}

        if original_text:
            properties["åŸå§‹æ–‡å­—"] = {"rich_text": [{"text": {"content": original_text[:2000]}}]}

        if keywords:
            properties["é—œéµå­—"] = {"multi_select": [{"name": kw[:100]} for kw in keywords[:10]]}

        if target_language:
            properties["ç›®æ¨™èªè¨€"] = {"select": {"name": target_language}}

        if user_id:
            properties["LINE ç”¨æˆ¶"] = {"rich_text": [{"text": {"content": user_id}}]}

        # Create page in database
        notion_client.pages.create(
            parent={"database_id": NOTION_DATABASE_ID},
            properties=properties
        )

        print(f"[DEBUG] Saved to Notion: {title[:50]}...")
        return True

    except Exception as e:
        print(f"[DEBUG] Notion save error: {str(e)}")
        return False


@app.route("/callback", methods=["POST"])
def callback():
    signature = request.headers.get("X-Line-Signature", "")
    body = request.get_data(as_text=True)

    try:
        handler.handle(body, signature)
    except InvalidSignatureError:
        abort(400)

    return "OK"


def translate_text(text: str, target_language: str) -> str:
    """Use OpenAI to translate text to target language"""
    if not openai_client:
        return "ç¿»è­¯åŠŸèƒ½æœªè¨­å®šï¼Œè«‹è¨­å®š OPENAI_API_KEY"

    try:
        prompt = f"""è«‹å°‡ä»¥ä¸‹æ–‡å­—ç¿»è­¯æˆ{target_language}ï¼š

{text}

æ³¨æ„äº‹é …ï¼š
1. åªéœ€è¦è¼¸å‡ºç¿»è­¯çµæœï¼Œä¸è¦åŠ ä»»ä½•è§£é‡‹æˆ–èªªæ˜
2. ä¿æŒåŸæ–‡çš„èªæ°£å’Œé¢¨æ ¼
3. å¦‚æœæœ‰å°ˆæœ‰åè©ï¼Œè«‹ä½¿ç”¨ç•¶åœ°å¸¸ç”¨çš„ç¿»è­¯æ–¹å¼
"""
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": f"ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç¿»è­¯åŠ©æ‰‹ï¼Œæ“…é•·å°‡å„ç¨®èªè¨€ç¿»è­¯æˆ{target_language}ã€‚åªè¼¸å‡ºç¿»è­¯çµæœï¼Œä¸åŠ ä»»ä½•é¡å¤–èªªæ˜ã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=2000,
            temperature=0.3
        )
        return response.choices[0].message.content

    except Exception as e:
        return f"ç¿»è­¯å¤±æ•—ï¼š{str(e)}"


def parse_translation_request(text: str) -> tuple[str, str] | None:
    """Parse translation request and return (target_language, text_to_translate)"""
    match = TRANSLATE_PATTERN.match(text.strip())
    if not match:
        return None

    language_input = match.group(1).strip()
    text_to_translate = match.group(2).strip()

    # Look up the target language
    target_language = LANGUAGE_MAP.get(language_input)

    # If not found in map, use the input directly (let OpenAI handle it)
    if not target_language:
        target_language = language_input

    return (target_language, text_to_translate)


def summarize_text(text: str) -> str:
    """Use OpenAI to summarize text content"""
    if not openai_client:
        return "æ–‡å­—æ‘˜è¦åŠŸèƒ½æœªè¨­å®šï¼Œè«‹è¨­å®š OPENAI_API_KEY"

    try:
        prompt = f"""è«‹åˆ†æä»¥ä¸‹æ–‡å­—å…§å®¹ï¼Œç”¨ç¹é«”ä¸­æ–‡æä¾›å®Œæ•´æ‘˜è¦ï¼š

{text}

è«‹ç”¨ä»¥ä¸‹æ ¼å¼å›è¦†ï¼ˆæ¯å€‹æ¬„ä½åªå¡«ä¸€å€‹å€¼ï¼‰ï¼š

ğŸ·ï¸ åˆ†é¡ï¼š[åªé¸ä¸€å€‹ï¼šç§‘æŠ€ã€AIã€å•†æ¥­ã€æ–°èã€æ•™å­¸ã€ç”Ÿæ´»ã€å¨›æ¨‚ã€ç­†è¨˜ã€æƒ³æ³•ã€å…¶ä»–]

ğŸ“Œ ä¸»é¡Œï¼š[ä¸€å¥è©±æè¿°æ ¸å¿ƒä¸»é¡Œ]

ğŸ“ é‡é»æ‘˜è¦ï¼š
â€¢ [é‡é»1 - è©³ç´°èªªæ˜]
â€¢ [é‡é»2 - è©³ç´°èªªæ˜]
â€¢ [é‡é»3 - è©³ç´°èªªæ˜]
ï¼ˆä¾å…§å®¹æä¾›3-5å€‹é‡é»ï¼‰

ğŸ”‘ é—œéµå­—ï¼š[åˆ—å‡º3-5å€‹é—œéµå­—ï¼Œç”¨é “è™Ÿåˆ†éš”ï¼Œä¾‹å¦‚ï¼šäººå·¥æ™ºæ…§ã€ç¨‹å¼è¨­è¨ˆã€è‡ªå‹•åŒ–]

ğŸ¯ ä¸€å¥è©±ç¸½çµï¼š[ç”¨ä¸€å¥è©±ç¸½çµæ•´æ®µæ–‡å­—çš„æ ¸å¿ƒå…§å®¹]
"""
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡å­—æ‘˜è¦åŠ©æ‰‹ï¼Œæ“…é•·æå–é‡é»ã€åˆ†é¡å…§å®¹ï¼Œä¸¦ç”¨ç¹é«”ä¸­æ–‡æ¸…æ™°å‘ˆç¾ã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1500,
            temperature=0.7
        )
        return response.choices[0].message.content

    except Exception as e:
        return f"æ‘˜è¦ç”Ÿæˆå¤±æ•—ï¼š{str(e)}"


@handler.add(MessageEvent, message=TextMessageContent)
def handle_text_message(event):
    """Handle text messages - translation, URL summary, or text summary"""
    with ApiClient(configuration) as api_client:
        line_bot_api = MessagingApi(api_client)

        text = event.message.text.strip()
        user_id = event.source.user_id
        print(f"[DEBUG] Received text: {text}, user_id: {user_id}")

        # Check if user is in translation mode (waiting for content to translate)
        if user_id in user_states and user_states[user_id].get("mode") == "translate_waiting":
            target_language = user_states[user_id].get("target_language")
            print(f"[DEBUG] User in translation mode, translating to: {target_language}")

            # Check if user wants to exit translation mode
            if text in ["å–æ¶ˆ", "é›¢é–‹", "çµæŸ", "exit", "cancel"]:
                del user_states[user_id]
                line_bot_api.reply_message_with_http_info(
                    ReplyMessageRequest(
                        reply_token=event.reply_token,
                        messages=[TextMessage(text="å·²é›¢é–‹ç¿»è­¯æ¨¡å¼ ğŸ‘‹")],
                    )
                )
                return

            # Check if user wants to switch language
            if text in ["ç¿»è­¯", "ç¿»è­¯æ¨¡å¼", "æ›èªè¨€", "åˆ‡æ›èªè¨€"]:
                user_states[user_id] = {"mode": "translate_select_language", "entered_at": time.time()}
                quick_reply_items = [
                    QuickReplyItem(action=MessageAction(label=label, text=label))
                    for label, _ in QUICK_REPLY_LANGUAGES
                ]
                quick_reply_items.append(
                    QuickReplyItem(action=MessageAction(label="âŒ å–æ¶ˆ", text="å–æ¶ˆ"))
                )
                line_bot_api.reply_message_with_http_info(
                    ReplyMessageRequest(
                        reply_token=event.reply_token,
                        messages=[TextMessage(
                            text="ğŸŒ åˆ‡æ›èªè¨€\n\nè«‹é¸æ“‡è¦ç¿»è­¯æˆçš„èªè¨€ï¼š\n\nğŸ’¡ ä¹Ÿå¯ä»¥ç›´æ¥è¼¸å…¥èªè¨€åç¨±ï¼ˆå¦‚ï¼šéŸ“æ–‡ã€é¦¬ä¾†æ–‡ï¼‰",
                            quick_reply=QuickReply(items=quick_reply_items)
                        )],
                    )
                )
                return

            # Translate the content
            try:
                translated = translate_text(text, target_language)
                # Keep user in translation mode for continuous translation
                # Reset timeout on each translation
                user_states[user_id]["entered_at"] = time.time()
                line_bot_api.reply_message_with_http_info(
                    ReplyMessageRequest(
                        reply_token=event.reply_token,
                        messages=[TextMessage(
                            text=f"ğŸŒ ç¿»è­¯çµæœï¼ˆ{target_language}ï¼‰\n\n{translated}\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€\nğŸ’¡ ç¹¼çºŒè¼¸å…¥æ–‡å­—å¯æŒçºŒç¿»è­¯\nè¼¸å…¥ã€Œå–æ¶ˆã€é›¢é–‹ç¿»è­¯æ¨¡å¼",
                            quick_reply=QuickReply(items=[
                                QuickReplyItem(action=MessageAction(label="ğŸšª é›¢é–‹ç¿»è­¯æ¨¡å¼", text="å–æ¶ˆ")),
                                QuickReplyItem(action=MessageAction(label="ğŸ”„ åˆ‡æ›èªè¨€", text="åˆ‡æ›èªè¨€")),
                            ])
                        )],
                    )
                )
                print(f"[DEBUG] Translation in mode sent successfully")

                # Save to Notion
                save_to_notion(
                    title=f"ç¿»è­¯ï¼š{text[:50]}...",
                    content_type="ç¿»è­¯",
                    category="ç¿»è­¯",
                    content=translated,
                    original_text=text,
                    target_language=target_language,
                    user_id=user_id
                )
            except Exception as e:
                print(f"[DEBUG] Translation error: {str(e)}")
                line_bot_api.reply_message_with_http_info(
                    ReplyMessageRequest(
                        reply_token=event.reply_token,
                        messages=[TextMessage(text=f"âŒ ç¿»è­¯å¤±æ•—ï¼š{str(e)}")],
                    )
                )
            return

        # Check if user selected a language from Quick Reply
        if user_id in user_states and user_states[user_id].get("mode") == "translate_select_language":
            # Check if the input matches a language
            selected_language = LANGUAGE_MAP.get(text)
            if selected_language:
                user_states[user_id] = {"mode": "translate_waiting", "target_language": selected_language, "entered_at": time.time()}
                line_bot_api.reply_message_with_http_info(
                    ReplyMessageRequest(
                        reply_token=event.reply_token,
                        messages=[TextMessage(text=f"âœ… å·²é¸æ“‡ç¿»è­¯æˆã€{text}ã€‘\n\nè«‹è¼¸å…¥è¦ç¿»è­¯çš„å…§å®¹ï¼š\n\nğŸ’¡ è¼¸å…¥ã€Œå–æ¶ˆã€å¯é›¢é–‹ç¿»è­¯æ¨¡å¼")],
                    )
                )
                print(f"[DEBUG] Language selected: {selected_language}")
                return
            # If input doesn't match a language, treat it as content to translate with default
            # Or show error - let's show the language selection again
            if text not in ["å–æ¶ˆ", "é›¢é–‹", "çµæŸ", "exit", "cancel"]:
                # Check if it's a valid language name not in our quick reply but in the map
                for lang_name, lang_code in LANGUAGE_MAP.items():
                    if text == lang_name:
                        user_states[user_id] = {"mode": "translate_waiting", "target_language": lang_code, "entered_at": time.time()}
                        line_bot_api.reply_message_with_http_info(
                            ReplyMessageRequest(
                                reply_token=event.reply_token,
                                messages=[TextMessage(text=f"âœ… å·²é¸æ“‡ç¿»è­¯æˆã€{text}ã€‘\n\nè«‹è¼¸å…¥è¦ç¿»è­¯çš„å…§å®¹ï¼š\n\nğŸ’¡ è¼¸å…¥ã€Œå–æ¶ˆã€å¯é›¢é–‹ç¿»è­¯æ¨¡å¼")],
                            )
                        )
                        return

        # Check if user wants to enter translation mode (just "ç¿»è­¯" or "ç¿»è­¯æ¨¡å¼")
        if text in ["ç¿»è­¯", "ç¿»è­¯æ¨¡å¼"]:
            user_states[user_id] = {"mode": "translate_select_language", "entered_at": time.time()}
            quick_reply_items = [
                QuickReplyItem(action=MessageAction(label=label, text=label))
                for label, _ in QUICK_REPLY_LANGUAGES
            ]
            # Add cancel option
            quick_reply_items.append(
                QuickReplyItem(action=MessageAction(label="âŒ å–æ¶ˆ", text="å–æ¶ˆ"))
            )

            line_bot_api.reply_message_with_http_info(
                ReplyMessageRequest(
                    reply_token=event.reply_token,
                    messages=[TextMessage(
                        text="ğŸŒ ç¿»è­¯æ¨¡å¼\n\nè«‹é¸æ“‡è¦ç¿»è­¯æˆçš„èªè¨€ï¼š\n\nğŸ’¡ ä¹Ÿå¯ä»¥ç›´æ¥è¼¸å…¥èªè¨€åç¨±ï¼ˆå¦‚ï¼šéŸ“æ–‡ã€é¦¬ä¾†æ–‡ï¼‰",
                        quick_reply=QuickReply(items=quick_reply_items)
                    )],
                )
            )
            print(f"[DEBUG] Entered translation mode, showing language selection")
            return

        # Check if user wants to cancel (outside of translation mode)
        if text in ["å–æ¶ˆ", "é›¢é–‹", "çµæŸ", "exit", "cancel"]:
            if user_id in user_states:
                del user_states[user_id]
            line_bot_api.reply_message_with_http_info(
                ReplyMessageRequest(
                    reply_token=event.reply_token,
                    messages=[TextMessage(text="å·²å–æ¶ˆ ğŸ‘‹")],
                )
            )
            return

        # Check if message is a direct translation request (ç¿»è­¯æˆè‹±æ–‡ï¼šä½ å¥½)
        translation_request = parse_translation_request(text)
        if translation_request:
            target_language, text_to_translate = translation_request
            print(f"[DEBUG] Translation request - Language: {target_language}, Text: {text_to_translate[:50]}...")

            try:
                translated = translate_text(text_to_translate, target_language)
                line_bot_api.reply_message_with_http_info(
                    ReplyMessageRequest(
                        reply_token=event.reply_token,
                        messages=[TextMessage(text=f"ğŸŒ ç¿»è­¯çµæœï¼ˆ{target_language}ï¼‰\n\n{translated}")],
                    )
                )
                print(f"[DEBUG] Translation sent successfully")

                # Save to Notion
                save_to_notion(
                    title=f"ç¿»è­¯ï¼š{text_to_translate[:50]}...",
                    content_type="ç¿»è­¯",
                    category="ç¿»è­¯",
                    content=translated,
                    original_text=text_to_translate,
                    target_language=target_language,
                    user_id=user_id
                )
            except Exception as e:
                print(f"[DEBUG] Translation error: {str(e)}")
                line_bot_api.reply_message_with_http_info(
                    ReplyMessageRequest(
                        reply_token=event.reply_token,
                        messages=[TextMessage(text=f"âŒ ç¿»è­¯å¤±æ•—ï¼š{str(e)}")],
                    )
                )
            return

        # Check if user is in scrape_waiting_count mode (waiting for post count)
        if user_id in user_states and user_states[user_id].get("mode") == "scrape_waiting_count":
            state = user_states[user_id]
            url = state.get("url")
            platform = state.get("platform")

            # Check for cancel
            if text in ["å–æ¶ˆ", "é›¢é–‹", "çµæŸ", "exit", "cancel"]:
                del user_states[user_id]
                line_bot_api.reply_message_with_http_info(
                    ReplyMessageRequest(
                        reply_token=event.reply_token,
                        messages=[TextMessage(text="å·²å–æ¶ˆçˆ¬å– ğŸ‘‹")],
                    )
                )
                return

            # Check if input is a number
            if text.isdigit():
                max_posts = min(int(text), 20)  # Cap at 20
                del user_states[user_id]  # Clear state

                if not apify_client:
                    line_bot_api.reply_message_with_http_info(
                        ReplyMessageRequest(
                            reply_token=event.reply_token,
                            messages=[TextMessage(text="âŒ ç¤¾ç¾¤çˆ¬èŸ²åŠŸèƒ½æœªè¨­å®šï¼Œè«‹è¨­å®š APIFY_API_KEY")],
                        )
                    )
                    return

                # Send initial response with clear wait time expectation
                line_bot_api.reply_message_with_http_info(
                    ReplyMessageRequest(
                        reply_token=event.reply_token,
                        messages=[TextMessage(text=f"ğŸ”„ é–‹å§‹çˆ¬å– {max_posts} ç¯‡è²¼æ–‡\n\nâ±ï¸ é è¨ˆéœ€è¦ 2-5 åˆ†é˜\nğŸ“± å®Œæˆå¾Œæœƒè‡ªå‹•é€šçŸ¥ä½ \n\nè«‹è€å¿ƒç­‰å€™ï¼Œä¸éœ€è¦é‡è¤‡ç™¼é€...")],
                    )
                )

                # Scrape multiple posts
                posts = scrape_facebook_post(url, max_posts) if platform == "facebook" else scrape_threads_post(url, max_posts)

                if not posts:
                    with ApiClient(configuration) as api_client2:
                        messaging_api2 = MessagingApi(api_client2)
                        messaging_api2.push_message(
                            PushMessageRequest(
                                to=user_id,
                                messages=[TextMessage(text=f"âŒ ç„¡æ³•çˆ¬å–è²¼æ–‡ï¼Œå¯èƒ½æ˜¯ç§äººå¸³è™Ÿæˆ–ç¶²å€ç„¡æ•ˆ")]
                            )
                        )
                    return

                # Process each post
                platform_name = "Facebook" if platform == "facebook" else "Threads"
                saved_count = 0

                for i, post_data in enumerate(posts):
                    try:
                        print(f"[DEBUG] Processing post {i+1}, raw data keys: {post_data.keys()}")
                        normalized_data = normalize_social_post_data(post_data, platform)
                        print(f"[DEBUG] Normalized: likes={normalized_data.get('likes')}, comments={normalized_data.get('comments')}")
                        summary = summarize_social_post(normalized_data, platform)
                        parsed = parse_social_summary_response(summary)

                        post_url = post_data.get("url") or post_data.get("postUrl") or url

                        if save_social_to_notion(
                            platform=platform_name,
                            username=normalized_data.get("username", "æœªçŸ¥"),
                            summary=parsed.get("summary", ""),
                            original_text=normalized_data.get("text", ""),
                            keywords=parsed.get("keywords", []),
                            likes=normalized_data.get("likes", 0),
                            comments=normalized_data.get("comments", 0),
                            shares=normalized_data.get("shares", 0),
                            source_url=post_url,
                            post_type=parsed.get("post_type", "å…¶ä»–"),
                            user_id=user_id
                        ):
                            saved_count += 1
                    except Exception as e:
                        print(f"[DEBUG] Error processing post {i+1}: {str(e)}")

                # Send completion message
                with ApiClient(configuration) as api_client2:
                    messaging_api2 = MessagingApi(api_client2)
                    messaging_api2.push_message(
                        PushMessageRequest(
                            to=user_id,
                            messages=[TextMessage(text=f"âœ… å®Œæˆï¼å·²çˆ¬å– {len(posts)} ç¯‡è²¼æ–‡ï¼ŒæˆåŠŸå­˜å…¥ Notion {saved_count} ç¯‡")]
                        )
                    )
                return

        # Check for multi-post scraping command: "çˆ¬ 5 ç¯‡ [URL]"
        multi_match = SCRAPE_MULTI_PATTERN.match(text)
        if multi_match:
            max_posts = min(int(multi_match.group(1)), 20)  # Cap at 20 posts
            url = multi_match.group(2)
            print(f"[DEBUG] Multi-post scraping: {max_posts} posts from {url}")

            if not apify_client:
                line_bot_api.reply_message_with_http_info(
                    ReplyMessageRequest(
                        reply_token=event.reply_token,
                        messages=[TextMessage(text="âŒ ç¤¾ç¾¤çˆ¬èŸ²åŠŸèƒ½æœªè¨­å®šï¼Œè«‹è¨­å®š APIFY_API_KEY")],
                    )
                )
                return

            platform, url_type = detect_social_platform(url)
            if not platform:
                line_bot_api.reply_message_with_http_info(
                    ReplyMessageRequest(
                        reply_token=event.reply_token,
                        messages=[TextMessage(text="âŒ ä¸æ”¯æ´çš„ç¶²å€æ ¼å¼ï¼Œè«‹æä¾› Facebook æˆ– Threads ç¶²å€")],
                    )
                )
                return

            # Send initial response
            line_bot_api.reply_message_with_http_info(
                ReplyMessageRequest(
                    reply_token=event.reply_token,
                    messages=[TextMessage(text=f"ğŸ”„ é–‹å§‹çˆ¬å– {max_posts} ç¯‡è²¼æ–‡\n\nâ±ï¸ é è¨ˆéœ€è¦ 2-5 åˆ†é˜\nğŸ“± å®Œæˆå¾Œæœƒè‡ªå‹•é€šçŸ¥ä½ \n\nè«‹è€å¿ƒç­‰å€™ï¼Œä¸éœ€è¦é‡è¤‡ç™¼é€...")],
                )
            )

            # Scrape multiple posts
            if platform == "facebook":
                posts = scrape_facebook_post(url, max_posts)
            else:
                posts = scrape_threads_post(url, max_posts)

            if not posts:
                # Use push message since we already replied
                with ApiClient(configuration) as api_client2:
                    messaging_api2 = MessagingApi(api_client2)
                    messaging_api2.push_message(
                        PushMessageRequest(
                            to=user_id,
                            messages=[TextMessage(text=f"âŒ ç„¡æ³•çˆ¬å–è²¼æ–‡ï¼Œå¯èƒ½æ˜¯ç§äººå¸³è™Ÿæˆ–ç¶²å€ç„¡æ•ˆ")]
                        )
                    )
                return

            # Process each post
            platform_name = "Facebook" if platform == "facebook" else "Threads"
            saved_count = 0

            for i, post_data in enumerate(posts):
                try:
                    normalized_data = normalize_social_post_data(post_data, platform)
                    summary = summarize_social_post(normalized_data, platform)
                    parsed = parse_social_summary_response(summary)

                    # Get post URL if available
                    post_url = post_data.get("url") or post_data.get("postUrl") or url

                    # Save to Notion
                    if save_social_to_notion(
                        platform=platform_name,
                        username=normalized_data.get("username", "æœªçŸ¥"),
                        summary=parsed.get("summary", ""),
                        original_text=normalized_data.get("text", ""),
                        keywords=parsed.get("keywords", []),
                        likes=normalized_data.get("likes", 0),
                        comments=normalized_data.get("comments", 0),
                        shares=normalized_data.get("shares", 0),
                        source_url=post_url,
                        post_type=parsed.get("post_type", "å…¶ä»–"),
                        user_id=user_id
                    ):
                        saved_count += 1
                        print(f"[DEBUG] Saved post {i+1}/{len(posts)}")
                except Exception as e:
                    print(f"[DEBUG] Error processing post {i+1}: {str(e)}")

            # Send completion message
            with ApiClient(configuration) as api_client2:
                messaging_api2 = MessagingApi(api_client2)
                messaging_api2.push_message(
                    PushMessageRequest(
                        to=user_id,
                        messages=[TextMessage(text=f"âœ… å®Œæˆï¼å·²çˆ¬å– {len(posts)} ç¯‡è²¼æ–‡ï¼ŒæˆåŠŸå­˜å…¥ Notion {saved_count} ç¯‡")]
                    )
                )
            return

        # Check if message contains a URL
        url = extract_url(text)
        print(f"[DEBUG] Extracted URL: {url}")

        if url:
            try:
                # Priority 1: Check if it's a social media URL (Facebook or Threads)
                platform, url_type = detect_social_platform(url)
                if platform:
                    print(f"[DEBUG] Detected {platform} {url_type} URL, scraping post...")

                    # Check if Apify is configured
                    if not apify_client:
                        line_bot_api.reply_message_with_http_info(
                            ReplyMessageRequest(
                                reply_token=event.reply_token,
                                messages=[TextMessage(text="âŒ ç¤¾ç¾¤çˆ¬èŸ²åŠŸèƒ½æœªè¨­å®šï¼Œè«‹è¨­å®š APIFY_API_KEY")],
                            )
                        )
                        return

                    # If it's a page URL, ask user how many posts to scrape
                    if url_type == "page":
                        # Store state for waiting scrape count
                        user_states[user_id] = {
                            "mode": "scrape_waiting_count",
                            "url": url,
                            "platform": platform,
                            "entered_at": time.time()
                        }
                        line_bot_api.reply_message_with_http_info(
                            ReplyMessageRequest(
                                reply_token=event.reply_token,
                                messages=[TextMessage(
                                    text=f"ğŸ“˜ åµæ¸¬åˆ° Facebook ç²‰å°ˆ/å€‹äººé é¢\n\nè«‹é¸æ“‡è¦çˆ¬å–å¹¾ç¯‡è²¼æ–‡ï¼š",
                                    quick_reply=QuickReply(items=[
                                        QuickReplyItem(action=MessageAction(label="3 ç¯‡", text="3")),
                                        QuickReplyItem(action=MessageAction(label="5 ç¯‡", text="5")),
                                        QuickReplyItem(action=MessageAction(label="10 ç¯‡", text="10")),
                                        QuickReplyItem(action=MessageAction(label="20 ç¯‡", text="20")),
                                        QuickReplyItem(action=MessageAction(label="âŒ å–æ¶ˆ", text="å–æ¶ˆ")),
                                    ])
                                )],
                            )
                        )
                        return

                    # Single post - scrape and analyze
                    posts = scrape_facebook_post(url, 1) if platform == "facebook" else scrape_threads_post(url, 1)

                    if not posts:
                        line_bot_api.reply_message_with_http_info(
                            ReplyMessageRequest(
                                reply_token=event.reply_token,
                                messages=[TextMessage(text=f"âŒ ç„¡æ³•çˆ¬å– {platform.title()} è²¼æ–‡ï¼Œå¯èƒ½æ˜¯ç§äººè²¼æ–‡æˆ–ç¶²å€ç„¡æ•ˆ")],
                            )
                        )
                        return

                    post_data = posts[0]

                    # Normalize data
                    normalized_data = normalize_social_post_data(post_data, platform)
                    print(f"[DEBUG] Normalized data: {normalized_data}")

                    # Generate AI summary
                    summary = summarize_social_post(normalized_data, platform)
                    parsed = parse_social_summary_response(summary)

                    # Build response message
                    platform_emoji = "ğŸ“˜" if platform == "facebook" else "ğŸ§µ"
                    platform_name = "Facebook" if platform == "facebook" else "Threads"

                    response_text = f"{platform_emoji} {platform_name} è²¼æ–‡åˆ†æ\n{url}\n\n{summary}"

                    line_bot_api.reply_message_with_http_info(
                        ReplyMessageRequest(
                            reply_token=event.reply_token,
                            messages=[TextMessage(text=response_text)],
                        )
                    )
                    print(f"[DEBUG] Social post analysis sent successfully")

                    # Save to Notion
                    save_social_to_notion(
                        platform=platform_name,
                        username=normalized_data.get("username", "æœªçŸ¥"),
                        summary=parsed.get("summary", ""),
                        original_text=normalized_data.get("text", ""),
                        keywords=parsed.get("keywords", []),
                        likes=normalized_data.get("likes", 0),
                        comments=normalized_data.get("comments", 0),
                        shares=normalized_data.get("shares", 0),
                        source_url=url,
                        post_type=parsed.get("post_type", "å…¶ä»–"),
                        user_id=user_id
                    )
                    return

                # Priority 2: Check if it's a Google Maps URL
                is_google_maps = any(pattern in url.lower() for pattern in [
                    'maps.google.com', 'google.com/maps', 'goo.gl/maps',
                    'maps.app.goo.gl', '/maps/', 'maps.app'
                ])

                if is_google_maps:
                    print(f"[DEBUG] Detected Google Maps URL, trying Apify scraper first...")
                    place_data = scrape_google_maps(url)
                    if place_data:
                        scraped_info = format_google_maps_result(place_data)
                        # Use OpenAI to enhance the scraped data with analysis
                        summary = summarize_google_maps(scraped_info, url)
                    else:
                        # Fallback to webpage scraping
                        print(f"[DEBUG] Apify scraper failed, falling back to webpage fetch...")
                        content = fetch_webpage_content(url)
                        print(f"[DEBUG] Maps content length: {len(content)}")
                        summary = summarize_google_maps(content, url)
                else:
                    # Priority 3: General webpage
                    print(f"[DEBUG] Fetching webpage content...")
                    content = fetch_webpage_content(url)
                    print(f"[DEBUG] Content length: {len(content)}")

                    print(f"[DEBUG] Generating webpage summary...")
                    summary = summarize_webpage(content)
                print(f"[DEBUG] Summary: {summary[:100]}...")

                line_bot_api.reply_message_with_http_info(
                    ReplyMessageRequest(
                        reply_token=event.reply_token,
                        messages=[TextMessage(text=f"ğŸ”— ç¶²é æ‘˜è¦\n{url}\n\n{summary}")],
                    )
                )
                print(f"[DEBUG] Reply sent successfully")

                # Save to Notion
                parsed = parse_summary_response(summary)
                save_to_notion(
                    title=parsed["title"] or url[:50],
                    content_type="URLæ‘˜è¦",
                    category=parsed["category"],
                    content=summary,
                    source_url=url,
                    keywords=parsed["keywords"],
                    user_id=user_id
                )
            except Exception as e:
                print(f"[DEBUG] Error: {str(e)}")
                line_bot_api.reply_message_with_http_info(
                    ReplyMessageRequest(
                        reply_token=event.reply_token,
                        messages=[TextMessage(text=f"âŒ ç¶²é æ‘˜è¦å¤±æ•—ï¼š{str(e)}")],
                    )
                )
        else:
            # Summarize the text
            print(f"[DEBUG] Generating text summary...")
            try:
                summary = summarize_text(text)
                line_bot_api.reply_message_with_http_info(
                    ReplyMessageRequest(
                        reply_token=event.reply_token,
                        messages=[TextMessage(text=f"ğŸ“ æ–‡å­—æ‘˜è¦\n\n{summary}")],
                    )
                )
                print(f"[DEBUG] Text summary sent successfully")
            except Exception as e:
                print(f"[DEBUG] Error: {str(e)}")
                line_bot_api.reply_message_with_http_info(
                    ReplyMessageRequest(
                        reply_token=event.reply_token,
                        messages=[TextMessage(text=f"âŒ æ–‡å­—æ‘˜è¦å¤±æ•—ï¼š{str(e)}")],
                    )
                )


@handler.add(MessageEvent, message=AudioMessageContent)
def handle_audio_message(event):
    """Handle audio messages - transcribe and reply with text"""
    with ApiClient(configuration) as api_client:
        line_bot_api = MessagingApi(api_client)
        blob_api = MessagingApiBlob(api_client)

        # Check if OpenAI is configured
        if not openai_client:
            line_bot_api.reply_message_with_http_info(
                ReplyMessageRequest(
                    reply_token=event.reply_token,
                    messages=[TextMessage(text="èªéŸ³è½‰æ–‡å­—åŠŸèƒ½æœªè¨­å®šï¼Œè«‹è¨­å®š OPENAI_API_KEY")],
                )
            )
            return

        try:
            # Download audio content from LINE
            audio_content = blob_api.get_message_content(event.message.id)

            # Save to temporary file
            with tempfile.NamedTemporaryFile(suffix=".m4a", delete=False) as tmp_file:
                # Handle both bytes and iterator response
                if hasattr(audio_content, 'read'):
                    tmp_file.write(audio_content.read())
                elif hasattr(audio_content, '__iter__') and not isinstance(audio_content, bytes):
                    for chunk in audio_content:
                        tmp_file.write(chunk)
                else:
                    tmp_file.write(audio_content)
                tmp_file_path = tmp_file.name

            # Transcribe using OpenAI Whisper
            with open(tmp_file_path, "rb") as audio_file:
                transcription = openai_client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language="zh",  # Chinese, change if needed
                )

            # Clean up temp file
            os.unlink(tmp_file_path)

            # Check for hallucination
            result_text = transcription.text if transcription.text else ""

            if is_hallucination(result_text):
                line_bot_api.reply_message_with_http_info(
                    ReplyMessageRequest(
                        reply_token=event.reply_token,
                        messages=[TextMessage(text="âš ï¸ ç„¡æ³•è¾¨è­˜èªéŸ³å…§å®¹\n\nå¯èƒ½åŸå› ï¼š\nâ€¢ èªéŸ³å¤ªçŸ­æˆ–å¤ªæ¨¡ç³Š\nâ€¢ èƒŒæ™¯å™ªéŸ³å¤ªå¤§\nâ€¢ æ²’æœ‰éŒ„åˆ°è²éŸ³\n\nè«‹é‡æ–°éŒ„è£½èªéŸ³è¨Šæ¯ã€‚")],
                    )
                )
                return

            # Reply with transcription
            line_bot_api.reply_message_with_http_info(
                ReplyMessageRequest(
                    reply_token=event.reply_token,
                    messages=[TextMessage(text=f"ğŸ“ èªéŸ³è½‰æ–‡å­—ï¼š\n\n{result_text}")],
                )
            )

            # Save to Notion
            user_id = event.source.user_id
            save_to_notion(
                title=f"èªéŸ³è½‰æ–‡å­—ï¼š{result_text[:50]}...",
                content_type="èªéŸ³è½‰æ–‡å­—",
                category="ç­†è¨˜",
                content=result_text,
                user_id=user_id
            )

        except Exception as e:
            # Clean up temp file if exists
            if 'tmp_file_path' in locals():
                try:
                    os.unlink(tmp_file_path)
                except:
                    pass

            line_bot_api.reply_message_with_http_info(
                ReplyMessageRequest(
                    reply_token=event.reply_token,
                    messages=[TextMessage(text=f"èªéŸ³è½‰æ–‡å­—å¤±æ•—ï¼š{str(e)}")],
                )
            )


# Initialize Notion social database on startup
if notion_client and NOTION_SOCIAL_DATABASE_ID:
    try:
        setup_notion_social_database()
    except Exception as e:
        print(f"[DEBUG] Social database init error: {str(e)}")


if __name__ == "__main__":
    port = int(os.getenv("PORT", 8000))
    app.run(host="0.0.0.0", port=port, debug=True)
